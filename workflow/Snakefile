import pathlib
import os

from scripts import zenodo
from scripts import helpers as hpr
from snakemake.utils import validate
from snakemake.io import strip_wildcard_constraints, expand
import pandas as pd

# Global conda container
base_container = 'docker://votti/miniconda3mamba:mamba0.3.0'

# Cellprofiler/Ilastik rules
include: 'rules/cellprofiler.smk'
include: 'rules/ilastik.smk'

# Read Configuration
configfile: 'config/config_bf.yml'
validate(config, "schemas/config_bf.schema.yml")

# Extract variables from configuration
## Input/output
input_data_folders = config['input_data_folders']
input_file_regexp = config['input_file_regexp']
input_zenodo_bf_repositories = config['input_zenodo_bf']

folder_base = pathlib.Path('results')


## Ilastik run config
ilastik_container = config['ilastik_container']

# Cellprofiler default config
cellprofiler_container = config['cellprofiler_container']
cp_plugins = config['cellprofiler_plugins']
# Define hardcoded variables
## Define basic folder structrue
folder_resources = pathlib.Path('resources')
fn_dat_input = folder_resources / 'input_files.csv'
fn_cp_scaling =  folder_resources / 'cp_pipelines/1_rescale_bf.cppipe'
fn_cp_segmentation =  folder_resources / 'cp_pipelines/2_segment_bf.cppipe'
fn_cp_measurement =  folder_resources / 'cp_pipelines/3_measure_bf.cppipe'
fn_ilastik_proj = folder_resources / 'classifiers/bfsegmentation.ilp'

fn_manual_qc = 'resources/bfqc_manual_v1.csv'

folder_analysis = folder_base / 'tiff'
folder_scaled = folder_analysis / ('scaled')
folder_probab = folder_analysis / ('probab')
folder_mask = folder_analysis / ('mask')
folder_cp = folder_base / ('cpout')
folder_cp_ov = folder_cp / 'overlays'

## Define Output files
fn_image = folder_cp / 'Image.csv'
fn_mask = folder_cp / 'mask.csv'
fn_experiment = folder_cp / 'Experiment.csv'

# Produce a list of all cellprofiler output files
cp_meas_output = [fn_image, fn_mask, fn_experiment]


## Define suffixes
suffix_plate = '_p{platenr}'
suffix_scale = '_r5'
suffix_mask = '_mask'
suffix_probablities = '_Probabilities'
suffix_overlay = '_overlay'
suffix_tiff = '.tiff'
suffix_done = '.done'

## Define derived file patterns
folder_input = folder_resources / 'bfimages'
folder_input.mkdir(exist_ok=True)
folder_zip = folder_resources / 'zips'
folder_zip.mkdir(exist_ok=True)
pat_fn_zip = folder_zip / ('{zipfol}')
pat_fn_unzip_done = folder_input / ('{zipfol}' + suffix_done)

done_rescale = folder_analysis / 'rescale.done'
fol_scale_combined =  folder_analysis / 'cp_rescale'/ 'combined'

FN_DICT = None
def get_dict():
    global FN_DICT
    if FN_DICT is None:
        dat = pd.read_csv(fn_dat_input)
        d = {str(out): str(inp) for inp, out in
             dat.loc[:, ['input_url', 'output_filename']].values}
        FN_DICT = d
    else:
        d = FN_DICT
    return d

# Define dynamic files
## Define (dynamic) input file functions
def fkt_fns_input(wildcards):
    """
    Identifies the input images.
    :param wildcards: wildcards dynamically provided by snakemake
    :return: A list of all `.ome.tiffs` generated.
    """
    checkpoints.define_input_files.get()
    fns_output = get_dict().keys()
    return expand(str(pat_fn_unzip_done), zipfol=fns_output)

# Define dynamic files
## Define (dynamic) input file functions

def get_zip_fn(wildcards):
    return get_dict()[wildcards.zipfol]

## Define derived (dynamic) input files functions
## This generates functions to define input filenames based on other input filename functions

# Analysis scripts
folder_results = pathlib.Path('results')
folder_plateov = folder_results / 'plate_overviews'
folder_wellov = folder_results / 'well_overviews'
fn_hqspheres = folder_results / 'hq_spheres.csv'
fn_fig1_bfspheres = folder_results / 'fig1_bfspheres.png'

# Output:
rule all:
    input: cp_meas_output, folder_cp_ov, folder_scaled, fn_hqspheres

rule scaled_imgs:
    input:
         folder_scaled
# Configuration for cellprofiler pipeline steps
# (Please look at rules/cellprofiler.smk for the documentation of this structure)
config_dict_cp = {
    'rescale': {
        'run_size': 50,
        'plugins': cp_plugins,
        'pipeline': str(fn_cp_scaling),
        'input_files': [folder_input, fkt_fns_input],
        'output_patterns': {'scaled': directory(folder_scaled)},
    },
    'segmasks': {
        'run_size': 50,
        'plugins': cp_plugins,
        'pipeline': str(fn_cp_segmentation),
        'input_files': [folder_probab],
        'output_patterns': {'.': directory(folder_mask)},
    },
    'measuremasks': {
        'run_size': 50,
        'plugins': cp_plugins,
        'pipeline': str(fn_cp_measurement),
        'input_files': [folder_mask, folder_scaled, folder_probab],
        'output_patterns': {'.': cp_meas_output, 'overlays': directory(folder_cp_ov)}
    }
}



# Configuration for Ilastik steps
# (Please look at rules/cellprofiler.smk for the documentation of this structure)
config_dict_ilastik = {
    'spheres':
        {'project': str(fn_ilastik_proj),
         'run_size': 50,
         'output_format': 'tiff',
         'output_filename': f'{{nickname}}{suffix_probablities}{suffix_tiff}',
         'export_source': 'Probabilities',
         'export_dtype': 'uint8',
         'pipeline_result_drange': '"(0.0, 1.0)"',
         'input_files': str(folder_scaled),
         'output_pattern': directory(folder_probab)
         }
}

# Target rules
checkpoint define_input_files:
    output: fn_dat_input
    run:
        dic_fn = zenodo.get_file_dicts(input_zenodo_bf_repositories)
        tup_fn = [(k, v) for k, v in dic_fn.items()]
        dat_input = pd.DataFrame(tup_fn, columns=['output_filename','input_url'])
        dat_input.to_csv(fn_dat_input, index=False)


rule retrieve_zenodo_bf:
    output:
          pat_fn_zip
    params:
          fn = get_zip_fn
    shell:
         'wget -P {folder_zip} {params.fn}'

rule unzip_bf_folder:
    input:
         fn_zip = pat_fn_zip
    output: touch(pat_fn_unzip_done)
    threads: 1
    params:
          fol_input = folder_input
    resources:
             mem_mb=lambda wildcards, attempt: (attempt * 4000),
             time='60'
    shell:
        'unzip -o {input.fn_zip} -d {params.fol_input}'

rule nb_sphere_size:
    input:
        fol_images=folder_scaled,
        fn_mask=fn_mask,
        fn_images=fn_image,
        fol_overviews=folder_cp_ov,
        fol_masks=folder_mask,
        fn_manual_qc=fn_manual_qc
    output:
        fol_plateov = directory(folder_plateov),
        fol_wellov = directory(folder_wellov),
        fn_hqspheres = fn_hqspheres,
        fn_fig1_bfspheres = fn_fig1_bfspheres
    conda:
        'envs/bf.yml'
    container: base_container
    threads: 8
    resources:
        mem='32G'
    log:
        # optional path to the processed notebook
        notebook="logs/notebooks/1_sphere_size.py.ipynb"
    notebook:
        "notebooks/1_sphere_size.py.ipynb"

fol_local = pathlib.Path('/mnt/scratch/vitoz/Git/SpheroidPublication')
fol_cluster = pathlib.Path('vizano@cluster.s3it.uzh.ch:/scratch/vizano/SpheroidPublication')
rule sync_to_cluster:
    shell:
         f'rsync -rtu {fol_local / "subworkflows/bf_preproc"} {fol_cluster / "subworkflows"} --progress --exclude=".*" --exclude="pkgs/" --exclude=".snakemake" --exclude="results/"'

rule sync_from_cluster:
    shell:
         f'rsync -rtu {fol_cluster / "subworkflows/bf_preproc"} {fol_local / "subworkflows"}  --progress --exclude=".*" --exclude="phys/resources" --exclude "phys/pkgs" --exclude="results/align_trakem2"'


## Rules to target Cellprofiler batch runs
define_cellprofiler_rules(config_dict_cp, folder_base, container_cp=cellprofiler_container)
define_ilastik_rules(config_dict_ilastik, folder_base, threads=4,
                     mem_mb=15000, container_ilastik=ilastik_container)

### Varia

rule clean:
    shell:
        "rm -R {folder_base}"